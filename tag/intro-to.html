<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="utf-8" />
    <title>Anything Technical | articles tagged "intro-to"</title>
    <link rel="shortcut icon" type="image/png" href="http://don-han.com/favicon.png">
    <link rel="shortcut icon" type="image/x-icon" href="http://don-han.com/favicon.ico">
    <link href="http://don-han.com/feeds/all.atom.xml" type="application/atom+xml" rel="alternate" title="Anything Technical Full Atom Feed" />
    <link rel="stylesheet" href="http://don-han.com/theme/css/screen.css" type="text/css" />
    <link rel="stylesheet" href="http://don-han.com/theme/css/pygments.css" type="text/css" />
    <link rel="stylesheet" href="http://don-han.com/theme/css/print.css" type="text/css" media="print" />
    <meta name="generator" content="Pelican" />
</head>
<body>
    <header>
        <nav>
            <ul>
                <li class="ephemeral selected"><a href="http://don-han.com/tag/intro-to.html">intro-to</a></li>
                <li><a href="http://don-han.com">Home</a></li>
                <li><a href="http://don-han.com/pages/about.html">about</a></li>
            </ul>
        </nav>
        <div class="header_box">
            <h1><a href="http://don-han.com">Anything Technical</a></h1>
        </div>
    </header>
    <div id="wrapper">
        <div id="content">            <h4 class="date">Aug 09, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://don-han.com/bash-parallel-programming.html" rel="bookmark" title="Permanent Link to &quot;[intro-to] Bash Parallel Programming&quot;">[intro-to] Bash Parallel Programming</a>
                </h2>

                <p>Parallel programming basically means writing scripts that can process multiple
tasks at the same time. This is in contrast to running scripts in a serial order which requires the
previous tasks to be completed before moving onto the next.</p>
<p>Parallelization can be useful for boosting the performance, but it requires the tasks
that are going to be parallelized to be independent of one another. Therefore, in
this tutorial, we will refer to the function <code>do-something</code> as a task that can
be safely run in parallel. For didactic purpose, we define <code>do-something</code> as follows:</p>
<div class="highlight"><pre><span class="k">function</span> <span class="k">do</span>-something<span class="o">(){</span>
    sleep <span class="nv">$1</span>
    <span class="nb">echo</span> <span class="nv">$1</span>
<span class="o">}</span>
</pre></div>


<p>There are several ways of parallelizing a bash script, and each has its own pros
and cons.</p>
<p>1.</p>
<div class="highlight"><pre><span class="k">for</span> i in <span class="k">$(</span>seq <span class="m">5</span> 1<span class="k">)</span><span class="p">;</span> <span class="k">do</span>
    <span class="k">do</span>-something <span class="nv">$i</span> <span class="p">&amp;</span>
<span class="k">done</span>
</pre></div>


<p>This is the simplest form of parallelization in bash. <code>&amp;</code> forks each
<code>do-something</code> task to the background every time it's called.</p>
<p>If you run this parallel script, your output should look something like this:</p>
<div class="highlight"><pre>1
2
3
4
5
</pre></div>


<p>Even though our script runs from 5 to 1, <code>do-something</code> task with the shortest sleeping time gets finished first.</p>
<p>Also, note the speed improvement. Our serial script runs in <code>5+4+3+2+1=15</code>
seconds while our parallel should theoretically run in 5 seconds. That's 3 times performance improvement by just attaching <code>&amp;</code>!</p>
<p>However, there is one critical problem. Imagine, instead of looping through 5 iterations, your script has to loop through 5000 iterations, and your <code>do-something</code> does something far more intensive. Then, your <code>&amp;</code> can easily overload the server. Our next method serves to prevent that.</p>
<p>ADVATAGE: simple</p>
<p>DISADVANTAGE: can potentially overload the system</p>
<p>2.</p>
<div class="highlight"><pre><span class="nv">wait_turn</span><span class="o">=</span>4
<span class="nv">j</span><span class="o">=</span>1
<span class="k">for</span> i in <span class="k">$(</span>seq <span class="m">10</span> 1<span class="k">)</span><span class="p">;</span> <span class="k">do</span>
    <span class="k">do</span>-something <span class="nv">$i</span> <span class="p">&amp;</span>
    <span class="k">if</span> <span class="o">((</span>j++ % <span class="nv">wait_turn</span> <span class="o">==</span> 0<span class="o">))</span><span class="p">;</span> <span class="k">then</span>
        <span class="nb">wait</span>
<span class="nb">    </span><span class="k">fi</span>
<span class="k">done</span>
<span class="nb">wait</span>
<span class="nb">echo</span> <span class="s2">&quot;after loop&quot;</span>
</pre></div>


<p>Running this, our result should look something like this:</p>
<div class="highlight"><pre>7
8
9
10
3
4
5
6
1
2
after loop
</pre></div>


<p>Can you see the pattern?</p>
<p>Once our <code>wait</code> function is called, the shell "waits" until all background tasks
are finished. In this case, a batch of four tasks are forked into the background at a time as specified by <code>wait_turn</code>, and the script waits until all four processes are complete. </p>
<p>The value of <code>wait_turn</code> has been chosen arbitrarily, so you can fiddle with it to find the right number for your own script. Usually, the value is a multiple of the number of the cores you have.</p>
<p>A keen observer might have noted the another <code>wait</code> function after the function.
To illustrate its need, consider a similar script without the after-loop <code>wait</code>:</p>
<div class="highlight"><pre><span class="nv">wait_turn</span><span class="o">=</span>4
<span class="nv">j</span><span class="o">=</span>1
<span class="k">for</span> i in <span class="k">$(</span>seq <span class="m">10</span> 1<span class="k">)</span><span class="p">;</span> <span class="k">do</span>
    <span class="k">do</span>-something <span class="nv">$i</span> <span class="p">&amp;</span>
    <span class="k">if</span> <span class="o">((</span>j++ % <span class="nv">wait_turn</span> <span class="o">==</span> 0<span class="o">))</span><span class="p">;</span> <span class="k">then</span>
        <span class="nb">wait</span>
<span class="nb">    </span><span class="k">fi</span>
<span class="k">done</span>
<span class="nb">echo</span> <span class="s2">&quot;after loop&quot;</span>
</pre></div>


<p>Before scrolling down, think what this would output.</p>
<div class="highlight"><pre>7
8
9
10
3
4
5
6
after loop
1
2
</pre></div>


<p>Notice how our "after loop" string comes before <code>do-something 1</code> and <code>do-something 2</code> is
processed. This could be critical if you have an after-loop code that needs the
for-loop to completely finish. Likewise, apply <code>wait</code> fucntion after the for-loop for the first method if necessary.</p>
<p>Yet, even this method can be restrictive. Consider a case where the total time
of task completion varies greatly. For our example above, say every fifth tasks
(<code>do-something 5</code>, <code>do-something 10</code>), for some odd reason, takes tremendous
amount of time to complete instead of mere 5 or 10 seconds, respectively. If we
run our script again under this new condition, we can see that our script runs
very inefficiently because for our first batch, even though tasks 7, 8, 9 are
already completed, they would have to wait for one task, <code>do-something 10</code> to
finish! Likewise, for our second batch, the threads that were working on tasks
3, 4, and 6 are idling because they cannot move forward until <code>do-something 5</code>
completes. In other words, if the batch contains an unusually slow task, it becomes
the bottle neck and slows down the entire process, almost defeating the purpose
of parallelism.</p>
<p>ADVANTAGE: parallelization without overloading the system</p>
<p>DISADVANTAGE: bottlenecks occur and can potentially slow down the entire process</p>
<p>3.</p>
<p>Fortunately, UNIX/LINUX comes with a command called
<a href="https://en.wikipedia.org/wiki/Xargs"><code>xargs</code></a> which whips idling tasks back to
work. . <code>xargs</code> is not specifically designed to be used for parallelization, but
it comes with a handy <code>-P</code> flag which defines the maximum number of processes that can run at
a time. <code>xargs</code> is extremely flexible, so I will not go into too much details in
this article, but I will cover the basics.</p>
<p>For example,</p>
<div class="highlight"><pre>seq <span class="m">10</span> <span class="m">1</span> <span class="p">|</span> xargs -n1 -P4 -I <span class="o">{}</span> bash -c <span class="s2">&quot;sleep {}; echo {};&quot;</span>
</pre></div>


<p>To break it down, I give the input ranging from 10 to 1. Then, <code>xargs</code> separates
the input into indiviual pieces (as specified by -n1, meaning take at most one
argument). The big <code>-P</code> flag as mentioned is what gives the parallelization
ability. Without it, the <code>xargs</code> runs the command in a serial manner. After we
set the variable to <code>{}</code>, <code>-I</code> replaces all ocurrences of <code>{}</code> in the command
with the input chunk, which in our case are individual numbers. Finally, <code>xargs</code>
runs the bash, which, in turn, executes the string with all <code>{}</code> replaced with
input chunks.</p>
<p>The above snippet produces:</p>
<div class="highlight"><pre>7
8
9
10
5
4
3
6
1
2
</pre></div>


<p>As we have seen with #2, since we have limited to the four processes, only upto 4 tasks are processed at a time. However, if you take a look at the second batch, you will see that the numbers are not ordered as before. This is because <code>xargs</code> assigns tasks as soon as processes becomes available.</p>
<p>Indeed, if we give as many processes as there are inputs, all processes will be handled at the same time as we would expect.</p>
<div class="highlight"><pre>seq <span class="m">10</span> <span class="m">1</span> <span class="p">|</span> xargs -n1 -P10 -I <span class="o">{}</span> bash -c <span class="s2">&quot;sleep {}; echo {};&quot;</span>
</pre></div>


<p>which rightly produces</p>
<div class="highlight"><pre>1
2
3
4
5
6
7
8
9
10
</pre></div>


<p><code>xargs</code> should be sufficient for most users under most usage cases. Other than the fact that it's a tad bit more complicated than the first two methods I introduced above, <code>xargs</code> provides an efficient, easy way of implementing a parallel script.</p>
<p>ADVANTAGE: eliminates bottleneck</p>
<p>DISADVANTAGE: none for most users. See #4 for more</p>
<p>4. </p>
<p>However, for some users, they might need something more powerful, in which case,
I introduce you <a href="http://www.gnu.org/software/parallel/">GNU Parallel</a>.
Thankfully, GNU Parallel already detailed
<a href="https://www.gnu.org/software/parallel/man.html#DIFFERENCES-BETWEEN-xargs-AND-GNU-Parallel">reasons why you might want to switch over</a>. </p>
<p>The <code>parallel</code> equivalent of </p>
<div class="highlight"><pre>seq <span class="m">10</span> <span class="m">1</span> <span class="p">|</span> xargs -P4 -I <span class="o">{}</span> bash -c <span class="s2">&quot;sleep {}; echo {};&quot;</span>
</pre></div>


<p>is</p>
<div class="highlight"><pre>seq <span class="m">10</span> <span class="m">1</span> <span class="p">|</span> parallel -P4 <span class="s2">&quot;sleep {}; echo {};&quot;</span>
</pre></div>


<p>You can see that <code>parallel</code>syntax is a lot simpler than <code>xargs</code>.</p>
<p>ADVANTAGE: refer to <a href="https://www.gnu.org/software/parallel/man.html#DIFFERENCES-BETWEEN-xargs-AND-GNU-Parallel">the comparison between xargs and GNU Parallel</a>; to name a few, flexibility and simplicity</p>
<p>DISADVANTAGE: external dependency</p>
                <div class="clear"></div>

                <div class="info">
                    &nbsp;&nbsp;<a href="http://don-han.com/category/articles.html" rel="tag">articles</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://don-han.com/tag/intro-to.html" class="tags selected">intro-to</a>
                    &nbsp;<a href="http://don-han.com/tag/bash.html" class="tags">bash</a>
                    &nbsp;<a href="http://don-han.com/tag/parallel-programming.html" class="tags">parallel programming</a>
                </div>
            </article>            <h4 class="date">May 30, 2015</h4>

            <article class="post">
                <h2 class="title">
                    <a href="http://don-han.com/intro-to-p-vs-np.html" rel="bookmark" title="Permanent Link to &quot;[intro-to] P vs. NP&quot;">[intro-to] P vs. NP</a>
                </h2>

                <p>NOTE: The following article is intended as a gentle introduction
for the subject, so I have intentionally left out some information that I considered
is out of scope. However, if you are familiar with the subject and if you think
I presented <em>factually wrong</em> statements, I will greatly appreciate an email
with corrections. Also, if you are confused with my explanation, please feel free to
send me an email with details of where you are confused, so I can make it a bit
more clear. </p>
<hr />
<h1>What exactly are P and NP? Why do we need them?</h1>
<p>In order to understand the P vs. NP problem, we must understand P and NP first. P
and NP rose from the necessity of matheticians and computer scientists to
describe how much resources, such as time, are needed to solve a decision
problem. A decision problem is defined as any problem that given an input can
determine or <em>decide</em> with "yes" or "no" answer. For example, given a path and a
budget cost as inputs, the decision problem might output "yes" if the path cost
is equal to or lower than the budget cost. Likewise, the decision problem might output "no" if the path cost is higher than the budget cost. </p>
<p>To be more precise, P problems are a set of decision problems that can be solved in polynomial
time <em>and</em> verified in polynomial time. For the decision problem used as an
example above, our verification algorithm may sum up the cost of edges in linear time ( O(n) ) to
compare it with the budget cost while the search algorithms such as DFS/BFS
obtain the path in linear time as wel. On the other hand, NP problems are
a set of decision problems which can be verified in polynomial time, but <em>not
necessarily</em> solved in polynomial time. </p>
<p>Although a common misunderstanding is to
conceptualize NP as "<em>not</em> P", such notion is greatly misleading. Note that NP
problems may or may not be solved in polynomial time. Therefore, NP and P are
not mutually exclusive classes. Rather, P is a subset of NP; all P problems
automatically qualify as NP problems since all P problems satisfy
polynomial-time verification <em>as well as</em> polynomial-time solution, and since NP
problems only need to fulfill polynomial-time verification, all P problems are NP problems as well.</p>
<p>For more technical details between P and NP, refer to the extra note.</p>
<h1>P versus NP</h1>
<p>Now we can tackle P vs. NP problem. The question simiply asks if P equals to NP
or not. In other words, the problem asks, if a
problem can be verified in a polynomial time, can the problem be solved in
a polynomial time <em>all the time</em>? That is, are some NP problems inherently
difficult or even impossible to solve in polynomial time, or have we just
not found the fast algorithms <em>yet</em>? So far, no official proof has been
recognized by the Clay Mathematics Institute, which promises to reward
one million dollars to anyone who can solve the P vs. NP problem. Although the question is simple, the implication is significant. </p>
<h1>Implications of P = NP</h1>
<p>If P = NP is proven, then this implies that all problems that can be
verified in polynomial time can also be solved in polynomial time as well. The
impact extends beyond the academic world since many industrial algorithms including
cryptography are written under the assumption that P != NP.</p>
<p>For example, RSA, which is a widely used cryptosystem to secure data
transmission, exploits P != NP by designing a decryption algorithm that runs in
polynomial time only for those who have the right access (known as a private key). If you
do not have the private key (meaning you do not have access), you would need to solve
the RSA problem, which is a NP problem since there is no known algorithm that
can decrypt encryption in polynomial time.</p>
<p>However, if P = NP is proven true, this implies that there exists some
algorithms that can solve the RSA problem in polynomial time, meaning any
encrypted message can be decrypted easily by even those who should not have
access if they can find the polynomial-time decryption algorithm, which is not
supposed to exist under the P != NP assumption. </p>
<p>Obviously, this is a significant security threat since this "encrpyted message"
could be a private message that you may not wish to share or even financial
information that can be hijacked.</p>
<p><strong>Therefore, if P = NP is true, many of the current algorithms that assume P !=
NP become insecure or invalid, and people can maliciously exploit the algorithm
for their personal gains.</strong></p>
<h1>Implications of P != NP</h1>
<p>If P != NP is true, there are some NP problems that will never be transition
into P, meaning there never will be algorithms that can solve those NP problems
in polynomial time.  </p>
<p>Although the impact of this proof will not be as significant as P = NP since most of the the academia and the industry already assume P != NP, the proof will influence how
researchers spend their time. <strong>Since there cannot be a
polynomial-time solution for some NP problems, researchers would no longer have
to waste their time trying to find fast algorithms for problems in NP.</strong></p>
<hr />
<h1>Extra Note</h1>
<p>NP stands for "non-deterministic polynomial time" and although P does
not have an official abbreviation, it is commonly known as "deterministic
polynomial time". A non-deterministic polynomial problem can be
solved in polynomial time with some lucky guesses. For example, finding a
least-cost cycle
that visits every node in a graph is known as a Travelling Salesman Problem,
which is notoriously difficult. However, if our solution algorithm makes certain
smart/lucky decisions in finding the path, the optimal path could be found in
polynomial time.</p>
<p>Meanwhile, the P problem can find a solution in polynomial time without depending on luck. As mentioned
before, since P is in NP under P != NP, P problems can also find polynomial-time
solution with some lucky guesses.</p>
                <div class="clear"></div>

                <div class="info">
                    &nbsp;&nbsp;<a href="http://don-han.com/category/articles.html" rel="tag">articles</a>
                    &nbsp;&middot;
                    &nbsp;<a href="http://don-han.com/tag/intro-to.html" class="tags selected">intro-to</a>
                    &nbsp;<a href="http://don-han.com/tag/theory-of-computer-science.html" class="tags">theory-of-computer-science</a>
                </div>
            </article>

            <div class="clear"></div>
            <footer>
                <p>
                <a href="https://github.com/jody-frankowski/blue-penguin">Blue Penguin</a> Theme
                &middot;
                Powered by <a href="http://getpelican.com">Pelican</a>
                &middot;
                <a href="http://don-han.com/feeds/all.atom.xml" rel="alternate">Atom Feed</a>
            </footer>
        </div>
        <div class="clear"></div>
    </div>
    <script type="text/javascript">
    var gaJsHost = (("https:" == document.location.protocol) ? "https://ssl." : "http://www.");
    document.write(unescape("%3Cscript src='" + gaJsHost + "google-analytics.com/ga.js' type='text/javascript'%3E%3C/script%3E"));
    </script>
    <script type="text/javascript">
    try {
        var pageTracker = _gat._getTracker("UA-73456201-1");
    pageTracker._trackPageview();
    } catch(err) {}</script>
</body>
</html>